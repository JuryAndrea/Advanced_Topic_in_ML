{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ2vJO49nF4A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy842Z9onF4B"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pcZxNc0nF4C"
      },
      "outputs": [],
      "source": [
        "N_HIDDEN = 64\n",
        "\n",
        "MAX_STEPS = 20\n",
        "VECTOR_LEN = 8\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "LAMBDA_P = 0.2\n",
        "BETA = 0.01\n",
        "\n",
        "LEARNING_RATE = 0.0003\n",
        "\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_H7LVrIrRHj"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC7-u7JfnF4C"
      },
      "source": [
        "### Setup data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parity task input comprises a vector consisting of $0s$, $1s$, and $-1s$.\n",
        "The output indicates the parity of $1s$ present; $1$ if there is an odd quantity and $0$ otherwise. The input is created by randomly assigning a random number of elements $[1,len)$ in a zeros vector as either $1$ or $-1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxgYH09XnF4D"
      },
      "outputs": [],
      "source": [
        "def get_data2(rows):\n",
        "    my_x = torch.randint(-1,2,(rows, VECTOR_LEN), dtype=torch.float)\n",
        "    my_y = ((my_x == 1.).sum(axis=1, dtype=torch.float) % 2).unsqueeze(1)\n",
        "\n",
        "    num_batches = rows // BATCH_SIZE\n",
        "    last_batch_size = rows % BATCH_SIZE\n",
        "\n",
        "    # Reshape the dataset into batches\n",
        "    batches_x = torch.tensor_split(my_x[:num_batches * BATCH_SIZE], num_batches)\n",
        "\n",
        "    batches_y = torch.tensor_split(my_y[:num_batches * BATCH_SIZE], num_batches)\n",
        "\n",
        "    if last_batch_size > 0:\n",
        "        batches_x = batches_x + tuple([my_x[num_batches * BATCH_SIZE:]])\n",
        "        batches_y = batches_y + tuple([my_y[num_batches * BATCH_SIZE:]])\n",
        "    return batches_x, batches_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLDDTTgbnF4D"
      },
      "outputs": [],
      "source": [
        "train_loader = get_data2(1_000_000)\n",
        "val_loader = get_data2(500_000)\n",
        "test_loader = get_data2(1_000_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BpEeRAgnF4D"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS5qMhsXnF4D"
      },
      "outputs": [],
      "source": [
        "class RecLoss(nn.Module):\n",
        "    # loss_func is the loss function for the original NN, cross-entropy in this case\n",
        "    def __init__(self, loss_fn: nn.Module):\n",
        "        super(RecLoss, self).__init__()\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, p, y_pred, y_true):\n",
        "        y_pred = y_pred.squeeze(2)\n",
        "        y_true = y_true.squeeze(1)\n",
        "        \n",
        "        total_loss = torch.stack([(self.loss_fn(y_pred_step, y_true) * p_step).mean() for y_pred_step, p_step in zip(y_pred,p)]).sum()\n",
        "        return total_loss\n",
        "\n",
        "def geometric_dist(steps, lam):\n",
        "    return lam * torch.pow((1-lam), torch.arange(1, steps+1) - 1)\n",
        "\n",
        "class RegLoss(nn.Module):\n",
        "    def __init__(self, lambda_p, max_n_steps):\n",
        "        super(RegLoss, self).__init__()\n",
        "        self.max_n_steps = max_n_steps\n",
        "        self.pg = geometric_dist(self.max_n_steps, lambda_p)\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, p):\n",
        "        pt = p.transpose(0,1)\n",
        "        cut_pg = self.pg[:pt.shape[1]].unsqueeze(0)\n",
        "\n",
        "        l = self.kl_div(pt.log(), cut_pg.expand_as(pt))\n",
        "        return l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTYx8e7PnF4D"
      },
      "source": [
        "### Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1NlrCYSnF4D"
      },
      "outputs": [],
      "source": [
        "class PonderParity(nn.Module):\n",
        "    def __init__(self, gru_cell, vector_len, max_steps, batch_size, lam_p, n_hidden, lr, device):\n",
        "        super(PonderParity, self).__init__()\n",
        "        self.gru_cell, self.input_vector_len, self.max_steps = gru_cell, vector_len, max_steps\n",
        "        self.batch_size, self.lam_p, self.n_hidden, self.learning_rate = batch_size, lam_p, n_hidden, lr\n",
        "        self._early_halt, self.device = False, device\n",
        "\n",
        "        # lambda layer (probablity of halting)\n",
        "        self.lam = nn.Linear(self.n_hidden, 1)\n",
        "\n",
        "        # output layer\n",
        "        self.out = nn.Linear(self.n_hidden, 1)\n",
        "\n",
        "        # losses\n",
        "        self.criterion = nn.BCEWithLogitsLoss(reduction='none').to(self.device)\n",
        "        self.rec_loss = RecLoss(self.criterion).to(self.device)\n",
        "        self.reg_loss = RegLoss(self.lam_p, self.max_steps).to(self.device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "\n",
        "    def halt_if_possible(self):\n",
        "        self._early_halt = True\n",
        "\n",
        "    def never_halt(self):\n",
        "        self._early_halt = False\n",
        "\n",
        "    def _dump_info(self):\n",
        "        print(f'''Training PonderParity with: \n",
        "            VectorLen: {self.input_vector_len}\n",
        "            MaxSteps: {self.max_steps}\n",
        "            LambdaP: {self.lam_p}\n",
        "            Hidden: {self.n_hidden}\n",
        "            LearningRate: {self.learning_rate}\\n''')\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.batch_size = x.shape[0]\n",
        "        h = torch.zeros((self.batch_size, self.n_hidden))\n",
        "\n",
        "        h = self.gru_cell(x, h)\n",
        "\n",
        "        lam_ns = np.zeros((self.batch_size, self.max_steps))\n",
        "\n",
        "        ps = np.zeros((self.max_steps, self.batch_size))\n",
        "\n",
        "        ys = []\n",
        "\n",
        "        halt_step = np.zeros(self.batch_size)\n",
        "\n",
        "        for step_n in range(1, self.max_steps + 1):\n",
        "\n",
        "            i = step_n - 1\n",
        "\n",
        "            # compute probability of halting at step_n and save value\n",
        "            # lam_n.shape == (batch_size,)\n",
        "            lam_n = torch.sigmoid(self.lam(h)).cpu().detach().numpy().flatten() if step_n < self.max_steps else np.ones((self.batch_size))\n",
        "\n",
        "            lam_ns[:,i] = lam_n\n",
        "            pn = lam_ns[:,i] * np.prod((1 - lam_ns[:,:i]), axis=1)\n",
        "            ps[i] = pn\n",
        "\n",
        "            # here the size of h becomes (self.batch_size, self.classes), where self.classes is the number of classes (10)\n",
        "            out = self.out(h)\n",
        "            ys.append(out)\n",
        "\n",
        "            # flip coin for each element of batch with probability lam_n (batch_size,)\n",
        "            # ... * (should_halt == 0) makes sure that earlier steps in which we deciced to halt is never replaced\n",
        "            should_halt = (np.random.rand(self.batch_size) < lam_n) * (halt_step == 0)\n",
        "\n",
        "            halt_step[should_halt] = step_n\n",
        "\n",
        "            h = self.gru_cell(x,h)\n",
        "\n",
        "            if self._early_halt and (not self.training) and halt_step.all():\n",
        "                break\n",
        "\n",
        "        return torch.tensor(ps, requires_grad=True),\\\n",
        "                torch.stack(ys),\\\n",
        "                torch.tensor(halt_step, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGgJa4OrnF4E"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, halted_step, labels, ponder):\n",
        "    corr_pred = 0\n",
        "    for s in range(halted_step.shape[0]):\n",
        "        if ponder:\n",
        "            corr_pred += ((y_pred[halted_step[s]-1][s] > 0.0).to(torch.float) == labels[s]).sum()\n",
        "        else:\n",
        "            corr_pred += ((y_pred[MAX_STEPS-1][s] > 0.0).to(torch.float) == labels[s]).sum()\n",
        "    return corr_pred / halted_step.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25-6Z6enF4E"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "GbSy8dkknF4E",
        "outputId": "81d58369-3364-4828-8559-150b7ef2ae65"
      },
      "outputs": [],
      "source": [
        "gru_cell = nn.GRUCell(VECTOR_LEN, N_HIDDEN).to(device)\n",
        "pnet = PonderParity(gru_cell, VECTOR_LEN, MAX_STEPS, BATCH_SIZE, LAMBDA_P, N_HIDDEN, LEARNING_RATE, device).to(device)\n",
        "pnet._dump_info()\n",
        "\n",
        "statistics = {'training_losses':[],\n",
        "                'validation_losses':[],\n",
        "                'validation_accuracies':[],\n",
        "                'separate_losses': []} # (rec, reg)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    sum_losses = .0\n",
        "    pnet.train()\n",
        "    for i, (inputs, labels) in enumerate(zip(train_loader[0],train_loader[1]), 1):\n",
        "        pnet.optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        p, y_pred, halted_step = pnet(inputs) \n",
        "\n",
        "        rec = pnet.rec_loss(p, y_pred, labels)\n",
        "        reg = pnet.reg_loss(p)\n",
        "        loss = rec + BETA * reg\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        pnet.optimizer.step()\n",
        "\n",
        "        statistics['training_losses'].append(loss.item())\n",
        "        statistics['separate_losses'].append((rec.item(), BETA * reg.item()))\n",
        "        sum_losses += loss.item()\n",
        "        print(f'Loss: {loss.item():.3f} | Rec: {rec.item():.3f} | Reg: {BETA * reg.item():.3f}', end='\\r')\n",
        "        if i % 500 == 0:\n",
        "            print(f'Epoch: {epoch+1} - Step: {i} | Train Loss: {sum_losses / 500:.3f}')\n",
        "            sum_losses = .0\n",
        "\n",
        "    pnet.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(zip(val_loader[0],val_loader[1]), 1):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            p, y_pred, halted_step = pnet(inputs)\n",
        "\n",
        "            val_acc = accuracy(y_pred, halted_step, labels, True)\n",
        "            statistics['validation_accuracies'].append(val_acc.item())\n",
        "\n",
        "            rec = pnet.rec_loss(p, y_pred, labels)\n",
        "            reg = pnet.reg_loss(p)\n",
        "            \n",
        "            val_loss = rec + BETA * reg\n",
        "\n",
        "            statistics['validation_losses'].append(val_loss.item())\n",
        "            print(f'Running Validation Loss: {val_loss.item():.3f}', end='\\r')\n",
        "            if val_loss.item() < best_loss:\n",
        "                best_loss = val_loss.item()\n",
        "                best_model_state = pnet.state_dict() \n",
        "\n",
        "    print(f'Epoch: {epoch+1} | Mean Val Loss: {np.array(statistics[\"validation_losses\"]).mean():.3f} | Mean Val Acc: {100 * np.array(statistics[\"validation_accuracies\"]).mean():.1f}%') \n",
        "    statistics[\"validation_accuracies\"].clear()\n",
        "    statistics[\"validation_losses\"].clear()\n",
        "\n",
        "if not os.path.exists('models_PARITY/'):\n",
        "    os.makedirs('models_PARITY/')\n",
        "torch.save(best_model_state, f'models_PARITY/parity_model_{LAMBDA_P}.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT9-4D_xnF4E"
      },
      "outputs": [],
      "source": [
        "# filename = ''\n",
        "# test_data = []\n",
        "# gru_cell2 = nn.GRUCell(VECTOR_LEN, N_HIDDEN).to(device)\n",
        "# model = PonderParity(gru_cell2, VECTOR_LEN, MAX_STEPS, BATCH_SIZE, LAMBDA_P, N_HIDDEN, LEARNING_RATE, device).to(device)\n",
        "\n",
        "# statistics = {'test_accuracies':[]}\n",
        "\n",
        "# model.load_state_dict(torch.load(filename))\n",
        "# model.eval() \n",
        "# model.halt_if_possible()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     accuracies = []\n",
        "#     hs_occurrences = np.zeros(MAX_STEPS)\n",
        "#     for i, (inputs, labels) in enumerate(zip(test_loader[0],test_loader[1]), 1):\n",
        "#         p, y_pred, halted_step = model(inputs)\n",
        "#         where, many = np.unique(halted_step, return_counts=True)\n",
        "#         hs_occurrences[where - 1] += many\n",
        "        \n",
        "#         test_acc = accuracy(y_pred, halted_step, labels, True)\n",
        "#         accuracies.append(test_acc.item())\n",
        "#         statistics['test_accuracies'].append(test_acc.item())\n",
        "        \n",
        "#     test_data.append(accuracies)\n",
        "#     fig, ax = plt.subplots(1,2,figsize=(10, 5))\n",
        "#     x = np.arange(1, MAX_STEPS+1)\n",
        "\n",
        "#     y = np.array((geometric_dist(MAX_STEPS, lp)))\n",
        "#     sns.barplot(x=x, y=y, ax=ax[0])\n",
        "#     ax[0].set_title(f\"Geometric Distribution for $\\lambda_p$ = {lp}\")\n",
        "#     ax[0].set_ylim(0,1)\n",
        "\n",
        "#     y = hs_occurrences/hs_occurrences.sum()\n",
        "#     sns.barplot(x=x, y=y, ax=ax[1])\n",
        "#     ax[1].set_title(f\"Real halt step distribution for $\\lambda_p$ = {lp}\")\n",
        "#     ax[1].set_ylim(0,1)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# print(f'Mean Test Acc: {100 * np.array(statistics[\"test_accuracies\"]).mean():.1f}%\\n')\n",
        "\n",
        "# test_data = np.array(test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
