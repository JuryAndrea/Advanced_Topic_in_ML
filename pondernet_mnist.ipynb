{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "\n",
    "N_OUT_CNN = 32\n",
    "N_OUT_MLP = 32\n",
    "N_HIDDEN_MLP = 32\n",
    "\n",
    "MAX_STEPS = 15\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LAMBDA_P = 0.2\n",
    "BETA = 0.05\n",
    "\n",
    "LEARNING_RATE = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_set, val_set = random_split(MNIST('./', download=True, train=True, transform=default_transform), [55000, 5000])\n",
    "test_set = MNIST('./', download=True, train=False, transform=default_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecLoss(nn.Module):\n",
    "    # loss_func is the loss function for the original NN, cross-entropy in this case\n",
    "    def __init__(self, loss_fn: nn.Module):\n",
    "        super(RecLoss, self).__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, p, y_pred, y_true):\n",
    "        total_loss = torch.stack([(self.loss_fn(y_pred_step, y_true) * p_step).mean() for y_pred_step, p_step in zip(y_pred,p)]).sum()\n",
    "        return total_loss\n",
    "\n",
    "def geometric_dist(steps, lam):\n",
    "    return lam * torch.pow((1-lam), torch.arange(1, steps+1) - 1)\n",
    "\n",
    "class RegLoss(nn.Module):\n",
    "    def __init__(self, lambda_p, max_n_steps):\n",
    "        super(RegLoss, self).__init__()\n",
    "        self.max_n_steps = max_n_steps\n",
    "        self.pg = geometric_dist(self.max_n_steps, lambda_p)\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, p):        \n",
    "        pt = p.transpose(0,1)\n",
    "        cut_pg = self.pg[:pt.shape[1]].unsqueeze(0)\n",
    "        l = self.kl_div(pt.log(), cut_pg.expand_as(pt)) \n",
    "        return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PonderCNN(nn.Module):\n",
    "    def __init__(self, mlp, cnn, mpl_out, classes, max_steps, batch_size, learning_rate, lam_p):\n",
    "        super(PonderCNN, self).__init__()\n",
    "        self.mlp_out, self.classes, self.mlp, self.cnn, self.max_steps = mpl_out, classes, mlp, cnn, max_steps\n",
    "        self.batch_size, self.learning_rate, self.lam_p = batch_size, learning_rate, lam_p\n",
    "        self._early_halt = False\n",
    "        \n",
    "        # lambda layer (probablity of halting)\n",
    "        self.lam = nn.Linear(self.mlp_out, 1)\n",
    "\n",
    "        # output layer\n",
    "        self.out = nn.Linear(self.mlp_out, self.classes)\n",
    "\n",
    "        # losses\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.rec_loss = RecLoss(self.criterion)\n",
    "        self.reg_loss = RegLoss(self.lam_p, self.max_steps)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def halt_if_possible(self):\n",
    "        self._early_halt = True\n",
    "\n",
    "    def never_halt(self):\n",
    "        self._early_halt = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_out = self.cnn(x)\n",
    "        self.batch_size = cnn_out.shape[0]\n",
    "        # send data through cnn\n",
    "        h = torch.zeros((self.batch_size, self.mlp_out))\n",
    "        \n",
    "        h = self.mlp(torch.concat([h, cnn_out], dim=1))\n",
    "\n",
    "        lam_ns = np.zeros((self.batch_size, self.max_steps))\n",
    "\n",
    "        ps = np.zeros((self.max_steps, self.batch_size))\n",
    "\n",
    "        ys = []\n",
    "\n",
    "        halt_step = np.zeros(self.batch_size)\n",
    "\n",
    "        for step_n in range(1, self.max_steps + 1):\n",
    "\n",
    "            i = step_n - 1\n",
    "            \n",
    "            # compute probability of halting at step_n and save value\n",
    "            # lam_n.shape = (batch_size,)\n",
    "            lam_n = torch.sigmoid(self.lam(h)).flatten().detach().numpy() if step_n < self.max_steps else np.ones((self.batch_size))\n",
    "\n",
    "            lam_ns[:,i] = lam_n\n",
    "            pn = lam_ns[:,i] * np.prod((1 - lam_ns[:,:i]), axis=1)\n",
    "            ps[i] = pn\n",
    "\n",
    "            # here the size of h becomes (self.batch_size, self.classes), where self.classes is the number of classes (10)\n",
    "            ys.append(self.out(h))\n",
    "\n",
    "            # flip coin for each element of batch with probability lam_n (batch_size,)\n",
    "            # ... * (should_halt == 0) makes sure that earlier step in which we deciced to halt is never replaced\n",
    "            should_halt = (np.random.rand(self.batch_size) < lam_n) * (halt_step == 0) \n",
    "            \n",
    "            halt_step[should_halt] = step_n\n",
    "\n",
    "            cnn_out = self.cnn(x)\n",
    "            h = self.mlp(torch.concat([h, cnn_out], dim=1))\n",
    "\n",
    "            if self._early_halt and (not self.training) and halt_step.all():\n",
    "                break\n",
    "\n",
    "        return torch.tensor(ps, requires_grad=True),\\\n",
    "                torch.stack(ys),\\\n",
    "                torch.tensor(halt_step, dtype=torch.long)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(MLP, self).__init__()\n",
    "        self.i2h = nn.Linear(n_input, n_hidden)\n",
    "        self.h2o = nn.Linear(n_hidden, n_output)\n",
    "        self.droput = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.i2h(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.h2o(x))\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input=28, n_output=50, kernel_size=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.lin_size = int(np.floor((np.floor((n_input - (kernel_size - 1)) / 2) - (kernel_size - 1)) / 2))\n",
    "        self.fc1 = nn.Linear(self.lin_size ** 2 * 20, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, halted_step, labels, ponder):\n",
    "    corr_pred = 0\n",
    "    for s in range(halted_step.shape[0]):\n",
    "        if ponder:\n",
    "            corr_pred += torch.argmax(y_pred[halted_step[s]-1][s]) == labels[s]\n",
    "        else:\n",
    "            corr_pred += torch.argmax(y_pred[MAX_STEPS-1][s]) == labels[s]\n",
    "    return corr_pred / halted_step.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(N_OUT_CNN+N_OUT_MLP, N_HIDDEN_MLP, N_OUT_MLP).to(device)\n",
    "cnn = CNN(n_input=28,n_output=N_OUT_CNN).to(device)\n",
    "\n",
    "pnet = PonderCNN(mlp, cnn, N_OUT_MLP, N_CLASSES, MAX_STEPS, BATCH_SIZE, LEARNING_RATE, LAMBDA_P).to(device)\n",
    "\n",
    "statistics = {'training_losses':[],\n",
    "                # 'training_accuracies':[],\n",
    "                'validation_losses':[],\n",
    "                'validation_accuracies':[],\n",
    "                'test_losses':[],\n",
    "                'test_accuracies':[]}\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(2):   \n",
    "    sum_losses = .0\n",
    "    pnet.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        pnet.optimizer.zero_grad()\n",
    "\n",
    "        p, y_pred, halted_step = pnet(inputs) \n",
    "\n",
    "\n",
    "        rec = pnet.rec_loss(p, y_pred, labels)\n",
    "        reg = pnet.reg_loss(p)\n",
    "        loss = rec + BETA * reg  \n",
    "\n",
    "        loss.backward()\n",
    "        pnet.optimizer.step()\n",
    "\n",
    "        statistics['training_losses'].append(loss.item())\n",
    "        sum_losses += loss.item()\n",
    "        print(f'Running Train Loss: {loss.item():.3f}', end='\\r')\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1} - Step: {i} | Train Loss: {sum_losses / 100:.3f}')\n",
    "            sum_losses = .0\n",
    "\n",
    "    pnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_loader, 1):\n",
    "            p, y_pred, halted_step = pnet(inputs)\n",
    "\n",
    "            val_acc = accuracy(y_pred, halted_step, labels, True)\n",
    "            statistics['validation_accuracies'].append(val_acc.item())\n",
    "\n",
    "            rec = pnet.rec_loss(p, y_pred, labels)\n",
    "            reg = pnet.reg_loss(p)\n",
    "            val_loss = rec + BETA * reg\n",
    "\n",
    "            statistics['validation_losses'].append(val_loss.item())\n",
    "            if val_loss.item() < best_loss:\n",
    "                best_loss = val_loss.item()\n",
    "                best_model_state = pnet.state_dict()\n",
    "\n",
    "    print(f'Epoch: {epoch+1} | Mean Val Loss: {np.array(statistics[\"validation_losses\"]).mean():.3f} | Mean Val Acc: {100 * np.array(statistics[\"validation_accuracies\"]).mean():.1f}%') \n",
    "\n",
    "pnet.eval()\n",
    "pnet.halt_if_possible()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "        p, y_pred, halted_step = pnet(inputs)  \n",
    "        \n",
    "        test_acc = accuracy(y_pred, halted_step, labels, True)\n",
    "        statistics['test_accuracies'].append(test_acc.item())\n",
    "        \n",
    "        rec = pnet.rec_loss(p, y_pred, labels)\n",
    "        reg = pnet.reg_loss(p)\n",
    "        test_loss = rec + BETA * reg\n",
    "\n",
    "        statistics['test_losses'].append(test_loss.item())\n",
    "\n",
    "print(f'Mean Test Loss: {np.array(statistics[\"test_losses\"]).mean():.3f} | Mean Test Acc: {100 * np.array(statistics[\"test_accuracies\"]).mean():.1f}%\\n')\n",
    "\n",
    "if not os.path.exists('models_MNIST/'):  \n",
    "    os.makedirs('models_MNIST/')\n",
    "torch.save(best_model_state, f'models_MNIST/MNIST_best_model_{LAMBDA_P}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = ''\n",
    "# test_data = []\n",
    "# mlp = MLP(N_OUT_CNN+N_OUT_MLP, N_HIDDEN_MLP, N_OUT_MLP).to(device)\n",
    "# cnn = CNN(n_input=28,n_output=N_OUT_CNN).to(device)\n",
    "# model = PonderCNN(mlp, cnn, N_OUT_MLP, N_CLASSES, MAX_STEPS, BATCH_SIZE, LEARNING_RATE, lp).to(device)\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load(filename))\n",
    "# model.eval()\n",
    "# model.halt_if_possible()\n",
    "# with torch.no_grad():\n",
    "#     accuracies = []\n",
    "#     hs_occurrences = np.zeros(MAX_STEPS)\n",
    "#     for i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "#         p, y_pred, halted_step = model(inputs)   \n",
    "        \n",
    "#         where, many = np.unique(halted_step, return_counts=True)\n",
    "#         hs_occurrences[where - 1] += many\n",
    "\n",
    "#         test_acc = accuracy(y_pred, halted_step, labels, True)\n",
    "#         accuracies.append(test_acc.item())\n",
    "\n",
    "#     test_data.append(accuracies)\n",
    "\n",
    "#     fig, ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "#     x = np.arange(1, MAX_STEPS+1)\n",
    "\n",
    "#     y = np.array((geometric_dist(MAX_STEPS, lp)))\n",
    "#     sns.barplot(x=x, y=y, ax=ax[0])\n",
    "#     ax[0].set_title(f\"Geometric Distribution for $\\lambda_p$ = {lp}\")\n",
    "#     ax[0].set_ylim(0,1)\n",
    "\n",
    "#     y = hs_occurrences/hs_occurrences.sum()\n",
    "#     sns.barplot(x=x, y=y, ax=ax[1])\n",
    "#     ax[1].set_title(f\"Real halt step distribution for $\\lambda_p$ = {lp}\")\n",
    "#     ax[1].set_ylim(0,1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# test_data = np.array(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
